{"hash":"82edfc9c503203a4b89aaf7ee02c1b0930a87be2","data":{"markdownPage":{"id":"1c9eb506c409cfe915a11380f5b9a649","title":"Kafka Connect","description":"","path":"/development/advanced-connect/","timeToRead":3,"content":"<h1 id=\"kafka-connect\"><a href=\"#kafka-connect\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Kafka Connect</h1>\n<h3 id=\"kafka-connect-1\"><a href=\"#kafka-connect-1\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Kafka Connect</h3>\n<ul>\n<li>Kafka Connect를 이용한 CDC(Change Data Capture)를 통해 데이터 동기화를 실습한다. </li>\n<li>Connect는 Connector를 실행시켜주는 서버로 DB동기화시, 벤더사가 만든 Connector, 또는 OSS(Debezium, Confluent) 계열의 Connector를 사용한다. </li>\n<li>Lab에서는 경량의 h2 DB를 사용한다.</li>\n</ul>\n<h4 id=\"connector-h2-database-다운로드\"><a href=\"#connector-h2-database-%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Connector, H2 database 다운로드</h4>\n<ul>\n<li>H2 DB와 Kafka Connect를 위한 JDBC 드라이브를 다운로드한다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">git clone https://github.com/acmexii/kafka-connect.git\ncd kakka-connect</code></pre>\n<ul>\n<li>h2-database 아카이브를 압축해제한다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">unzip h2.zip</code></pre>\n<h4 id=\"h2-데이터베이스-실행\"><a href=\"#h2-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4-%EC%8B%A4%ED%96%89\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>H2 데이터베이스 실행</h4>\n<ul>\n<li>bin 폴더로 이동해 h2 database를 서버모드로 실행한다. </li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">cd bin\nchmod 755 h2.sh\n./h2.sh -webPort 8087 -tcpPort 9099</code></pre>\n<ul>\n<li>지정한 webPort로 Client WebUI가 접근가능하며,  h2 database는 9099포트(default 9092)로 실행된다. </li>\n</ul>\n<h4 id=\"kafka-jdbc-connector-설치\"><a href=\"#kafka-jdbc-connector-%EC%84%A4%EC%B9%98\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Kafka JDBC Connector 설치</h4>\n<ul>\n<li>Jdbc Connector는 설치된 Kafka 서버에 등록하고 사용한다.</li>\n<li>Connector를 설치할 폴더를 생성한다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">cd $kafka_home\nmkdir connectors\ncd connectors</code></pre>\n<ul>\n<li>다운받은 confluentinc-kafka-connect-jdbc-10.2.5.zip을 복사 후 unzip 한다. </li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">cp /home/project/advanced-connect/kafka-connect/confluentinc-kafka-connect-jdbc-10.2.5.zip ./\nunzip confluentinc-kafka-connect-jdbc-10.2.5.zip</code></pre>\n<h4 id=\"connect-서버에-connector-등록\"><a href=\"#connect-%EC%84%9C%EB%B2%84%EC%97%90-connector-%EB%93%B1%EB%A1%9D\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Connect 서버에 Connector 등록</h4>\n<ul>\n<li>kafka Connect에 설치한 Confluent jdbc Connector를 등록한다.</li>\n<li>$kafka_home/config 폴더로 이동 후 connect-distributed.properties 파일 오픈하고,</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">cd $kafka_home/config \nvi connect-distributed.properties</code></pre>\n<ul>\n<li>마지막 행을 plugin.path=/usr/local/kafka/connectors 로 편집하고 저장종료한다. </li>\n</ul>\n<h4 id=\"kafka-connect-실행\"><a href=\"#kafka-connect-%EC%8B%A4%ED%96%89\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Kafka Connect 실행</h4>\n<ul>\n<li>$kafka_home에서 connect를 실행한다. </li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">cd $kafka_home\nbin/connect-distributed.sh config/connect-distributed.properties</code></pre>\n<ul>\n<li>Kafka Connect는 default 8083 포트로 실행이 된다. </li>\n<li>Labs > 포트확인 메뉴를 통해 실행중 서비스를 확인한다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">root@labs-315390334:/home/project# netstat -lntp | grep :808 \ntcp        0      0 0.0.0.0:8083            0.0.0.0:*               LISTEN      23597/java          \ntcp        0      0 0.0.0.0:8087            0.0.0.0:*               LISTEN      21885/java          \nroot@labs-315390334:/home/project# </code></pre>\n<ul>\n<li>Kafka topic을 확인해 본다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list</code></pre>\n<ul>\n<li>\n<p>Connect를 위한 토픽이 추가되었다.</p>\n<blockquote>\n<p>connect-configs, connect-offsets, connect-status</p>\n</blockquote>\n</li>\n</ul>\n<h4 id=\"source-connector-설치\"><a href=\"#source-connector-%EC%84%A4%EC%B9%98\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Source Connector 설치</h4>\n<ul>\n<li>Kafka connect의 REST API를 통해 Source 및 Sink connector를 등록한다. </li>\n</ul>\n<pre class=\"language-curl\"><code class=\"language-curl\">curl -i -X POST -H \"Accept:application/json\" \\\n    -H  \"Content-Type:application/json\" http://localhost:8083/connectors/ \\\n    -d '{\n    \"name\": \"h2-source-connect\",\n    \"config\": {\n        \"connector.class\": \"io.confluent.connect.jdbc.JdbcSourceConnector\",\n        \"connection.url\": \"jdbc:h2:tcp://localhost:9099/./test\",\n        \"connection.user\":\"sa\",\n        \"connection.password\":\"passwd\",\n        \"mode\":\"incrementing\",\n        \"incrementing.column.name\" : \"ID\",\n        \"table.whitelist\" : \"ORDER_TABLE\",\n        \"topic.prefix\" : \"CONNECT_\",\n        \"tasks.max\" : \"1\"\n    }\n}'</code></pre>\n<blockquote>\n<p>Connector 등록시, 'No suitable driver' 오류가 발생할 경우, Classpath에 h2 driver를 설정해 준다.\nh2/bin에 있는 JDBC 드라이브를 $kafka_home/lib에 복사하고 다시 Connect를 실행한다. </p>\n</blockquote>\n<ul>\n<li>등록한 Connector를 확인한다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">http localhost:8083/connectors</code></pre>\n<h4 id=\"order-마이크로서비스-설정\"><a href=\"#order-%EB%A7%88%EC%9D%B4%ED%81%AC%EB%A1%9C%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%84%A4%EC%A0%95\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Order 마이크로서비스 설정</h4>\n<ul>\n<li>주문 서비스를 서버모드로 실행한 h2 Database에 연결한다.</li>\n<li>Order의 application.yml을 열어 default profile의 datasource를 수정한다.</li>\n</ul>\n<pre class=\"language-yaml\"><code class=\"language-yaml\">  <span class=\"token key atrule\">datasource</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">url</span><span class=\"token punctuation\">:</span> jdbc<span class=\"token punctuation\">:</span>h2<span class=\"token punctuation\">:</span>tcp<span class=\"token punctuation\">:</span>//localhost<span class=\"token punctuation\">:</span>9099/./test\n    <span class=\"token key atrule\">username</span><span class=\"token punctuation\">:</span> sa\n    <span class=\"token key atrule\">password</span><span class=\"token punctuation\">:</span> passwd\n    <span class=\"token key atrule\">driverClassName</span><span class=\"token punctuation\">:</span> org.h2.Driver</code></pre>\n<h4 id=\"소스-테이블에-data-입력\"><a href=\"#%EC%86%8C%EC%8A%A4-%ED%85%8C%EC%9D%B4%EB%B8%94%EC%97%90-data-%EC%9E%85%EB%A0%A5\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>소스 테이블에 Data 입력</h4>\n<ul>\n<li>order 마이크로서비스를 기동하고 소스 테이블에 데이터를 생성한다.</li>\n</ul>\n<pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token builtin class-name\">cd</span> order\nmvn spring-boot:run\nhttp POST :8081/orders <span class=\"token assign-left variable\">message</span><span class=\"token operator\">=</span><span class=\"token string\">\"1st OrderPlaced.\"</span>\nhttp POST :8081/orders <span class=\"token assign-left variable\">message</span><span class=\"token operator\">=</span><span class=\"token string\">\"2nd OrderPlaced.\"</span></code></pre>\n<ul>\n<li>Kafka topic을 확인해 본다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list</code></pre>\n<ul>\n<li>\n<p>'CONNECT_ORDER_TABLE' 토픽이 추가되어 목록에 나타난다.</p>\n<blockquote>\n<p>Kafka Connect는 테이블 단위로 토픽이 생성되어 Provider와 Consumer간 데이터를 Sync합니다. </p>\n</blockquote>\n</li>\n</ul>\n<pre class=\"language-text\"><code class=\"language-text\">$kafka_home/bin/kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic CONNECT_ORDER_TABLE --from-beginning</code></pre>\n<h4 id=\"sink-connector-설치\"><a href=\"#sink-connector-%EC%84%A4%EC%B9%98\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Sink Connector 설치</h4>\n<pre class=\"language-curl\"><code class=\"language-curl\">curl -i -X POST -H \"Accept:application/json\" \\\n    -H  \"Content-Type:application/json\" http://localhost:8083/connectors/ \\\n    -d '{\n    \"name\": \"h2-sink-connect\",\n    \"config\": {\n        \"connector.class\": \"io.confluent.connect.jdbc.JdbcSinkConnector\",\n        \"connection.url\": \"jdbc:h2:tcp://localhost:9099/./test\",\n        \"connection.user\":\"sa\",\n        \"connection.password\":\"passwd\",\n        \"auto.create\":\"true\",       \n        \"auto.evolve\":\"true\",       \n        \"delete.enabled\":\"false\",\n        \"tasks.max\":\"1\",\n        \"topics\":\"CONNECT_ORDER_TABLE\"\n    }\n}'</code></pre>\n<h4 id=\"타겟-테이블-data-확인\"><a href=\"#%ED%83%80%EA%B2%9F-%ED%85%8C%EC%9D%B4%EB%B8%94-data-%ED%99%95%EC%9D%B8\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>타겟 테이블 Data 확인</h4>\n<ul>\n<li>Sync대상 테이블을 조회한다.</li>\n</ul>\n<pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token builtin class-name\">cd</span> order\nmvn spring-boot:run\nhttp GET :8081/syncOrders </code></pre>\n<blockquote>\n<p>Sink Connector를 통해 syncOrders 테이블에 복제된 데이터가 조회된다</p>\n</blockquote>\n<ul>\n<li>다시한번 Orders 테이블에 데이터를 입력하고 확인해 본다.</li>\n</ul>\n<pre class=\"language-bash\"><code class=\"language-bash\">http POST :8081/orders <span class=\"token assign-left variable\">message</span><span class=\"token operator\">=</span><span class=\"token string\">\"3rd OrderPlaced.\"</span>\nhttp GET :8081/syncOrders</code></pre>\n<h4 id=\"이기종간-dbms-연계\"><a href=\"#%EC%9D%B4%EA%B8%B0%EC%A2%85%EA%B0%84-dbms-%EC%97%B0%EA%B3%84\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>이기종간 DBMS 연계</h4>\n<ul>\n<li>Sink Connector의 JDBC  Url만 다른 DB정보로 설정하면, 이기종 DB간에도 데이터가 동기화가 가능하다.</li>\n</ul>\n","sidebar":"business","next":"","prev":"","headings":[{"depth":1,"value":"Kafka Connect","anchor":"#kafka-connect"},{"depth":3,"value":"Kafka Connect","anchor":"#kafka-connect-1"},{"depth":4,"value":"Connector, H2 database 다운로드","anchor":"#connector-h2-database-다운로드"},{"depth":4,"value":"H2 데이터베이스 실행","anchor":"#h2-데이터베이스-실행"},{"depth":4,"value":"Kafka JDBC Connector 설치","anchor":"#kafka-jdbc-connector-설치"},{"depth":4,"value":"Connect 서버에 Connector 등록","anchor":"#connect-서버에-connector-등록"},{"depth":4,"value":"Kafka Connect 실행","anchor":"#kafka-connect-실행"},{"depth":4,"value":"Source Connector 설치","anchor":"#source-connector-설치"},{"depth":4,"value":"Order 마이크로서비스 설정","anchor":"#order-마이크로서비스-설정"},{"depth":4,"value":"소스 테이블에 Data 입력","anchor":"#소스-테이블에-data-입력"},{"depth":4,"value":"Sink Connector 설치","anchor":"#sink-connector-설치"},{"depth":4,"value":"타겟 테이블 Data 확인","anchor":"#타겟-테이블-data-확인"},{"depth":4,"value":"이기종간 DBMS 연계","anchor":"#이기종간-dbms-연계"}]},"allMarkdownPage":{"edges":[{"node":{"path":"/operations/ops-service-mesh-istio/","title":"[Service Mesh] Istio"}},{"node":{"path":"/operations/ops-utility/","title":"쿠버네티스 유틸리티"}},{"node":{"path":"/operations/ops-readiness/","title":"무정지 배포 실습"}},{"node":{"path":"/operations/ops-ingress-virtualhost/","title":"Ingress - Virtual Host based"}},{"node":{"path":"/operations/ops-persistence-volume/","title":"파일시스템 (볼륨) 연결과 데이터베이스 설정"}},{"node":{"path":"/operations/ops-liveness/","title":"셀프힐링 실습"}},{"node":{"path":"/operations/ops-ingress/","title":"Ingress 를 통한 진입점 통일 - Path-based routing"}},{"node":{"path":"/operations/ops-kubernetes/","title":"Kubernetes Basic Commands"}},{"node":{"path":"/operations/ops-deploy-my-app/","title":"애플리케이션 패키징,도커라이징,클러스터 배포"}},{"node":{"path":"/operations/ops-aws-setting/","title":"AWS Cloud Setup(EKS, ECR 설정)"}},{"node":{"path":"/operations/ops-autoscale/","title":"Pod Auto Scaling"}},{"node":{"path":"/operations/ops-argo-rollout-canary-istio/","title":"[GitOps] Argo Rollout 와 Istio 를 통한 카나리 배포"}},{"node":{"path":"/operations/k8s-monitoring/","title":"MSA 모니터링 with installing Grafana"}},{"node":{"path":"/operations/msa-logging/","title":"MSA 로깅 with EFK Stack"}},{"node":{"path":"/operations/ops-anatomy-kubernetes/","title":"쿠버네티스 내부구조 분석"}},{"node":{"path":"/operations/istio-resiliency-part2/","title":"[Service Mesh] Istio 를 통한 서비스 회복성 Part2 - 서킷브레이커"}},{"node":{"path":"/operations/istio-msa-telemetry/","title":"[Service Mesh] MSA 모니터링 w/ Istio addon Grafana"}},{"node":{"path":"/operations/istio-traffic/","title":"[Service Mesh] Istio 를 통한 동적 트래픽 라우팅"}},{"node":{"path":"/operations/istio-resiliency-part1/","title":"[Service Mesh] Istio 를 통한 서비스 회복성 Part1 - 타임아웃/재시도"}},{"node":{"path":"/operations/gitops-argo-cd/","title":"[GitOps] Argo CD 를 통한 카나리 배포"}},{"node":{"path":"/operations/azure/","title":"Azure Cloud Setup (AKS, ACR 설정)"}},{"node":{"path":"/development/oauth2withkeycloak/","title":"JWT토큰 기반 인증인가 w/ Keycloak Authz-svr"}},{"node":{"path":"/operations/end-to-end/","title":"12번가 전체 마이크로서비스의 배포"}},{"node":{"path":"/development/ops-docker/","title":"[빌드] Docker Image Build & Push"}},{"node":{"path":"/development/oauth2/","title":"JWT토큰 기반 인증인가 w/ Spring Authz-svr"}},{"node":{"path":"/development/monolith2misvc/","title":"[구현] Req/Res 방식의 MSA 연동"}},{"node":{"path":"/development/keycloak-oauth2-2/","title":"Keycloak OIDC w/ OAuth2 Client"}},{"node":{"path":"/development/kafka-scaling/","title":"Kafka 스케일링"}},{"node":{"path":"/development/keycloak-oauth2-3/","title":"Fine grained RBAC w/ Resource Server"}},{"node":{"path":"/development/keycloak-oauth2-1/","title":"Keycloak Authorization 서버 설정"}},{"node":{"path":"/development/gateway/","title":"[구현] 게이트웨이를 통한 진입점 통일"}},{"node":{"path":"/development/kafka-base/","title":"[pre-lab] 카프카 연습"}},{"node":{"path":"/development/kafka-retry-dlq/","title":"Retry & Dead Letter Queue"}},{"node":{"path":"/development/kafka-manual-commit/","title":"Kafka 수동커밋"}},{"node":{"path":"/development/front-end/","title":"프론트엔드 개발"}},{"node":{"path":"/development/dp-cqrs/","title":"[구현] 데이터프로젝션-CQRS"}},{"node":{"path":"/development/dp-graphql/","title":"[구현] 데이터프로젝션-GraphQL"}},{"node":{"path":"/development/dp-composite-svc/","title":"[구현] 데이터프로젝션-컴포지트서비스"}},{"node":{"path":"/development/cqrs-modeling/","title":"[pre-lab] CQRS 샘플 모델링"}},{"node":{"path":"/development/cna-pubsub2/","title":"[구현] Pub/Sub - Compensation and Correlation"}},{"node":{"path":"/development/contract-test/","title":"[테스트] Consumer Driven Test 기반 Contract Test"}},{"node":{"path":"/development/cna-start/","title":"[구현] 마이크로서비스의 실행"}},{"node":{"path":"/development/capstone-project-2/","title":"[Capstone Prj.] Simple Mall - Implementation"}},{"node":{"path":"/development/cna-pubsub/","title":"[구현] Pub/Sub 방식의 MSA 연동"}},{"node":{"path":"/development/circuitbreaker/","title":"[구현] Req/Res 방식에서 장애전파 차단(서킷브레이커 패턴)"}},{"node":{"path":"/business/ddd-google-drive/","title":"[이벤트스토밍] DDD 구글 드라이브 예제"}},{"node":{"path":"/development/advanced-connect/","title":"Kafka Connect"}},{"node":{"path":"/business/zero-based-cna/","title":"[설계] ES모델 기반 Inner 아키텍처 이해"}},{"node":{"path":"/development/capstone-project-1/","title":"[Capstone Prj.] Simple Mall - Scenario/Modeling"}},{"node":{"path":"/business/collaborative-eventstorming/","title":"[이벤트스토밍] Collaborative Eventstorming"}},{"node":{"path":"/business/","title":"[분석] DDD 이벤트의 도출 - 12번가 쇼핑몰"}},{"node":{"path":"/business/eventstorming-fooddelivery/","title":"[이벤트스토밍] DDD Food Delivery 예제"}},{"node":{"path":"/business/design-to-code/","title":"[설계] ES모델기반 템플릿 코드 분석"}}]}},"context":{}}