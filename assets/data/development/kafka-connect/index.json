{"hash":"49f02bf9a1ddbf017efeb189302ebd145a2955d8","data":{"markdownPage":{"id":"59a15b326dd00387c07c16407776b927","title":"CDC(Change Data Capture) with Kafka","description":"","path":"/development/kafka-connect/","timeToRead":5,"content":"<h1 id=\"cdcchange-data-capture-with-kafka\"><a href=\"#cdcchange-data-capture-with-kafka\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>CDC(Change Data Capture) with Kafka</h1>\n<h1 id=\"cdcchange-data-capture-with-kafka-1\"><a href=\"#cdcchange-data-capture-with-kafka-1\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>CDC(Change Data Capture) with Kafka</h1>\n<h3 id=\"kafka-connect\"><a href=\"#kafka-connect\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Kafka Connect</h3>\n<ul>\n<li>Kafka Connect를 이용한 CDC(Change Data Capture)를 통해 주문팀에서 생성된 데이터가 추천상품을 위해, 패턴 분석이 필요한 마케팅팀에 동기화 되는지를 실습한다. </li>\n<li>Connect는 Connector를 실행시켜주는 서버로 DB동기화시, 벤더사가 만든 Connector, 또는 OSS(Debezium, Confluent) 계열의 Connector를 사용한다. </li>\n<li>Lab에서는 경량의 h2 DB를 사용한다.</li>\n</ul>\n<h4 id=\"connector-h2-database-다운로드\"><a href=\"#connector-h2-database-%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Connector, H2 database 다운로드</h4>\n<ul>\n<li>H2 DB와 Kafka Connect를 위한 JDBC 드라이브를 다운로드한다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">git clone https://github.com/acmexii/kafka-connect.git\ncd kafka-connect</code></pre>\n<ul>\n<li>h2-database 아카이브를 압축해제한다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">mkdir ./h2\nunzip h2.zip ./h2/</code></pre>\n<h4 id=\"h2-데이터베이스-실행\"><a href=\"#h2-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4-%EC%8B%A4%ED%96%89\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>H2 데이터베이스 실행</h4>\n<ul>\n<li>bin 폴더로 이동해 h2 database를 서버모드로 실행한다. </li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">cd ./h2/bin\nchmod 755 h2.sh\n./h2.sh -webPort 8087 -tcpPort 9099</code></pre>\n<ul>\n<li>지정한 webPort로 Client WebUI가 접근 가능하다.  </li>\n<li>h2 database는 9099포트(default 9092)로 실행된다. </li>\n</ul>\n<h4 id=\"kafka-설치-및-실행\"><a href=\"#kafka-%EC%84%A4%EC%B9%98-%EB%B0%8F-%EC%8B%A4%ED%96%89\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Kafka 설치 및 실행</h4>\n<p>새로운 터미널에서 kafka를 수동으로 설치한다.</p>\n<pre class=\"language-sh\"><code class=\"language-sh\">cd kafka-connect\ncurl \"https://archive.apache.org/dist/kafka/2.7.1/kafka_2.13-2.7.1.tgz\" -o ./kafka-2.7.1.tgz\ntar xvfz kafka-2.7.1.tgz\ncd kafka_2.13-2.7.1/\nbin/zookeeper-server-start.sh config/zookeeper.properties &</code></pre>\n<ul>\n<li>새로운 터미널에서 kafka 데몬을 실행한다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">cd kafka-connect\ncd kafka_2.13-2.7.1/\nbin/kafka-server-start.sh config/server.properties &</code></pre>\n<h4 id=\"kafka-jdbc-connector-설치\"><a href=\"#kafka-jdbc-connector-%EC%84%A4%EC%B9%98\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Kafka JDBC Connector 설치</h4>\n<ul>\n<li>Jdbc Connector를 설치된 Kafka 서버에 등록하고 사용한다.</li>\n<li>Connector를 설치할 폴더를 생성한다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">cd kafka-connect/kafka_2.13-2.7.1/\nexport kafka_home=$PWD\nmkdir connectors\ncd connectors</code></pre>\n<ul>\n<li>다운받은 confluentinc-kafka-connect-jdbc-10.2.5.zip을 복사 후 unzip 한다. </li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">cp ../../confluentinc-kafka-connect-jdbc-10.2.5.zip ./\nunzip confluentinc-kafka-connect-jdbc-10.2.5.zip</code></pre>\n<h4 id=\"connect-서버에-connector-등록\"><a href=\"#connect-%EC%84%9C%EB%B2%84%EC%97%90-connector-%EB%93%B1%EB%A1%9D\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Connect 서버에 Connector 등록</h4>\n<ul>\n<li>kafka Connect에 설치한 Confluent jdbc Connector를 등록한다.</li>\n<li>$kafka_home/config 폴더로 이동 후 connect-distributed.properties 파일 오픈하고,</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">cd $kafka_home/config \nvi connect-distributed.properties</code></pre>\n<ul>\n<li>마지막 행으로 이동하여 주석을 제거한다.</li>\n</ul>\n<pre class=\"language-text\"><code class=\"language-text\">plugin.path=/workspace/kafka-cdc/kafka-connect/kafka_2.13-2.7.1/connectors</code></pre>\n<ul>\n<li>위와 같이 편집하고 저장종료한다. </li>\n</ul>\n<h4 id=\"kafka-connect-서버-실행\"><a href=\"#kafka-connect-%EC%84%9C%EB%B2%84-%EC%8B%A4%ED%96%89\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Kafka Connect 서버 실행</h4>\n<ul>\n<li>$kafka_home에서 connect를 실행한다. </li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">cd $kafka_home\nbin/connect-distributed.sh config/connect-distributed.properties &</code></pre>\n<ul>\n<li>Kafka Connect는 default 8083 포트로 실행이 된다. </li>\n</ul>\n<ul>\n<li>Kafka topic을 확인해 본다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">$kafka_home/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list</code></pre>\n<ul>\n<li>\n<p>Connect를 위한 토픽이 추가되었다.</p>\n<blockquote>\n<p>connect-configs, connect-offsets, connect-status</p>\n</blockquote>\n</li>\n</ul>\n<h4 id=\"source-connector-설치\"><a href=\"#source-connector-%EC%84%A4%EC%B9%98\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Source Connector 설치</h4>\n<ul>\n<li>Kafka connect의 REST API를 통해 Source 및 Sink connector를 등록한다. </li>\n</ul>\n<pre class=\"language-curl\"><code class=\"language-curl\">curl -i -X POST -H \"Accept:application/json\" \\\n    -H  \"Content-Type:application/json\" http://localhost:8083/connectors/ \\\n    -d '{\n    \"name\": \"h2-source-connector\",\n    \"config\": {\n        \"connector.class\": \"io.confluent.connect.jdbc.JdbcSourceConnector\",\n        \"connection.url\": \"jdbc:h2:tcp://localhost:9099/./test\",\n        \"connection.user\":\"sa\",\n        \"connection.password\":\"passwd\",\n        \"mode\":\"incrementing\",\n        \"incrementing.column.name\" : \"ID\",\n        \"table.whitelist\" : \"ORDER_TABLE\",\n        \"topic.prefix\" : \"SYNC_\",\n        \"tasks.max\" : \"1\"\n    }\n}'</code></pre>\n<blockquote>\n<p>Connector 등록시, 'No suitable driver' 오류가 발생할 경우, Classpath에 h2 driver를 설정해 준다.\nh2/bin에 있는 JDBC 드라이브를 $kafka_home/lib에 복사하고 다시 Connect를 실행한다. </p>\n</blockquote>\n<ul>\n<li>등록한 Connector를 확인한다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">http localhost:8083/connectors</code></pre>\n<h4 id=\"order-마이크로서비스-설정\"><a href=\"#order-%EB%A7%88%EC%9D%B4%ED%81%AC%EB%A1%9C%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%84%A4%EC%A0%95\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Order 마이크로서비스 설정</h4>\n<ul>\n<li>주문 서비스를 h2 Database에 연결한다.</li>\n<li>Order의 application.yml을 열어 default profile의 datasource를 확인한다.</li>\n</ul>\n<pre class=\"language-yaml\"><code class=\"language-yaml\">  <span class=\"token key atrule\">datasource</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">url</span><span class=\"token punctuation\">:</span> jdbc<span class=\"token punctuation\">:</span>h2<span class=\"token punctuation\">:</span>tcp<span class=\"token punctuation\">:</span>//localhost<span class=\"token punctuation\">:</span>9099/./test\n    <span class=\"token key atrule\">username</span><span class=\"token punctuation\">:</span> sa\n    <span class=\"token key atrule\">password</span><span class=\"token punctuation\">:</span> passwd\n    <span class=\"token key atrule\">driverClassName</span><span class=\"token punctuation\">:</span> org.h2.Driver</code></pre>\n<h4 id=\"소스-테이블에-data-입력\"><a href=\"#%EC%86%8C%EC%8A%A4-%ED%85%8C%EC%9D%B4%EB%B8%94%EC%97%90-data-%EC%9E%85%EB%A0%A5\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>소스 테이블에 Data 입력</h4>\n<ul>\n<li>order 마이크로서비스를 기동하고 소스 테이블에 데이터를 생성한다.</li>\n</ul>\n<pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token builtin class-name\">cd</span> order\nmvn spring-boot:run\nhttp POST :8081/orders <span class=\"token assign-left variable\">productId</span><span class=\"token operator\">=</span><span class=\"token number\">1</span> <span class=\"token assign-left variable\">qty</span><span class=\"token operator\">=</span><span class=\"token number\">10</span> <span class=\"token assign-left variable\">customerId</span><span class=\"token operator\">=</span><span class=\"token number\">1000</span> <span class=\"token assign-left variable\">price</span><span class=\"token operator\">=</span><span class=\"token number\">10000</span></code></pre>\n<ul>\n<li>Kafka topic을 확인해 본다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">$kafka_home/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list</code></pre>\n<ul>\n<li>\n<p>'SYNC_ORDER_TABLE' 토픽이 추가되어 목록에 나타난다.</p>\n<blockquote>\n<p>Kafka Connect는 테이블 단위로 토픽이 생성되어 Provider와 Consumer간 데이터를 Sync합니다. </p>\n</blockquote>\n</li>\n</ul>\n<pre class=\"language-text\"><code class=\"language-text\">$kafka_home/bin/kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic SYNC_ORDER_TABLE --from-beginning</code></pre>\n<h4 id=\"sink-connector-설치\"><a href=\"#sink-connector-%EC%84%A4%EC%B9%98\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Sink Connector 설치</h4>\n<pre class=\"language-curl\"><code class=\"language-curl\">curl -i -X POST -H \"Accept:application/json\" \\\n    -H  \"Content-Type:application/json\" http://localhost:8083/connectors/ \\\n    -d '{\n    \"name\": \"h2-sink-connector\",\n    \"config\": {\n        \"connector.class\": \"io.confluent.connect.jdbc.JdbcSinkConnector\",\n        \"connection.url\": \"jdbc:h2:tcp://localhost:9099/./test\",\n        \"connection.user\":\"sa\",\n        \"connection.password\":\"passwd\",\n        \"auto.create\":\"true\",       \n        \"auto.evolve\":\"true\",       \n        \"delete.enabled\":\"false\",\n        \"tasks.max\":\"1\",\n        \"topics\":\"SYNC_ORDER_TABLE\"\n    }\n}'</code></pre>\n<h4 id=\"marketing-마이크로서비스-설정-및-실행\"><a href=\"#marketing-%EB%A7%88%EC%9D%B4%ED%81%AC%EB%A1%9C%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%84%A4%EC%A0%95-%EB%B0%8F-%EC%8B%A4%ED%96%89\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>marketing 마이크로서비스 설정 및 실행</h4>\n<ul>\n<li>마케팅 서비스를 h2 Database에 연결한다.</li>\n<li>marketing 서비스의 application.yml을 열어 default profile의 datasource를 확인한다.</li>\n</ul>\n<pre class=\"language-yaml\"><code class=\"language-yaml\">  <span class=\"token key atrule\">datasource</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">url</span><span class=\"token punctuation\">:</span> jdbc<span class=\"token punctuation\">:</span>h2<span class=\"token punctuation\">:</span>tcp<span class=\"token punctuation\">:</span>//localhost<span class=\"token punctuation\">:</span>9099/./test\n    <span class=\"token key atrule\">username</span><span class=\"token punctuation\">:</span> sa\n    <span class=\"token key atrule\">password</span><span class=\"token punctuation\">:</span> passwd\n    <span class=\"token key atrule\">driverClassName</span><span class=\"token punctuation\">:</span> org.h2.Driver</code></pre>\n<blockquote>\n<p>Sink Connector를 통해 주문서비스에서 입력한 정보가 CDC를 통해 마케팅 테이블(SYNC_ORDER_TABLE)에 복제된 데이터가 조회된다.</p>\n</blockquote>\n<ul>\n<li>다시한번 Orders 테이블에 데이터를 입력하고 마케팅팀에 주문 데이터 동기화가 되는지 확인해 본다.</li>\n</ul>\n<pre class=\"language-bash\"><code class=\"language-bash\">http POST :8081/orders <span class=\"token assign-left variable\">productId</span><span class=\"token operator\">=</span><span class=\"token number\">1</span> <span class=\"token assign-left variable\">qty</span><span class=\"token operator\">=</span><span class=\"token number\">10</span> <span class=\"token assign-left variable\">customerId</span><span class=\"token operator\">=</span><span class=\"token number\">1000</span> <span class=\"token assign-left variable\">price</span><span class=\"token operator\">=</span><span class=\"token number\">10000</span>\nhttp GET :8082/syncOrders</code></pre>\n<h4 id=\"이기종간-dbms-연계\"><a href=\"#%EC%9D%B4%EA%B8%B0%EC%A2%85%EA%B0%84-dbms-%EC%97%B0%EA%B3%84\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>이기종간 DBMS 연계</h4>\n<ul>\n<li>Sink Connector의 JDBC  Url만 다른 DB정보로 설정하여 Connect Server에 등록하면 이기종 DB간에도 데이터가 동기화가 가능해진다.</li>\n</ul>\n","sidebar":"business","next":"","prev":"","headings":[{"depth":1,"value":"CDC(Change Data Capture) with Kafka","anchor":"#cdcchange-data-capture-with-kafka"},{"depth":1,"value":"CDC(Change Data Capture) with Kafka","anchor":"#cdcchange-data-capture-with-kafka-1"},{"depth":3,"value":"Kafka Connect","anchor":"#kafka-connect"},{"depth":4,"value":"Connector, H2 database 다운로드","anchor":"#connector-h2-database-다운로드"},{"depth":4,"value":"H2 데이터베이스 실행","anchor":"#h2-데이터베이스-실행"},{"depth":4,"value":"Kafka 설치 및 실행","anchor":"#kafka-설치-및-실행"},{"depth":4,"value":"Kafka JDBC Connector 설치","anchor":"#kafka-jdbc-connector-설치"},{"depth":4,"value":"Connect 서버에 Connector 등록","anchor":"#connect-서버에-connector-등록"},{"depth":4,"value":"Kafka Connect 서버 실행","anchor":"#kafka-connect-서버-실행"},{"depth":4,"value":"Source Connector 설치","anchor":"#source-connector-설치"},{"depth":4,"value":"Order 마이크로서비스 설정","anchor":"#order-마이크로서비스-설정"},{"depth":4,"value":"소스 테이블에 Data 입력","anchor":"#소스-테이블에-data-입력"},{"depth":4,"value":"Sink Connector 설치","anchor":"#sink-connector-설치"},{"depth":4,"value":"marketing 마이크로서비스 설정 및 실행","anchor":"#marketing-마이크로서비스-설정-및-실행"},{"depth":4,"value":"이기종간 DBMS 연계","anchor":"#이기종간-dbms-연계"}]},"allMarkdownPage":{"edges":[{"node":{"path":"/operations/service/","title":"12번가 마이크로서비스 라우터(Service) 생성"}},{"node":{"path":"/operations/ops-service-mesh-istio/","title":"[Service Mesh] Istio"}},{"node":{"path":"/operations/ops-utility/","title":"쿠버네티스 유틸리티"}},{"node":{"path":"/operations/ops-persistence-volume/","title":"파일시스템 (볼륨) 연결과 데이터베이스 설정"}},{"node":{"path":"/operations/ops-readiness/","title":"셀프힐링 & 무정지 배포 실습"}},{"node":{"path":"/operations/ops-kubernetes/","title":"Kubernetes Basic Command"}},{"node":{"path":"/operations/ops-service-mesh-istio-2/","title":"[Service Mesh] Istio-2"}},{"node":{"path":"/operations/ops-pod-status/","title":"Pod 상태값에 따른 마이크로서비스 트러블 슈팅"}},{"node":{"path":"/operations/ops-persistence-volume-efs/","title":"파일공유를 위한 NAS 스토리지 생성과 설정"}},{"node":{"path":"/operations/ops-liveness/","title":"셀프힐링 실습"}},{"node":{"path":"/operations/ops-ingress/","title":"Ingress 를 통한 진입점 통일 - Path-based routing"}},{"node":{"path":"/operations/ops-aws-setting/","title":"AWS Cloud Setup(EKS, ECR 설정)"}},{"node":{"path":"/operations/ops-ingress-virtualhost/","title":"Ingress - Virtual Host based"}},{"node":{"path":"/operations/ops-deploy-my-app/","title":"애플리케이션 패키징,도커라이징,클러스터 배포"}},{"node":{"path":"/operations/ops-autoscale/","title":"Pod Auto Scaling"}},{"node":{"path":"/operations/istio-resiliency-part2/","title":"[Service Mesh] Istio 를 통한 서비스 회복성 Part2 - 서킷브레이커"}},{"node":{"path":"/operations/istio-resiliency-part1/","title":"[Service Mesh] Istio 를 통한 서비스 회복성 Part1 - 타임아웃/재시도"}},{"node":{"path":"/operations/ops-argo-rollout-canary-istio/","title":"[GitOps] Argo Rollout 와 Istio 를 통한 카나리 배포"}},{"node":{"path":"/operations/k8s-monitoring/","title":"MSA 모니터링 with installing Grafana"}},{"node":{"path":"/operations/microservice-logging/","title":"마이크로서비스 통합 로깅 with EFK stack"}},{"node":{"path":"/operations/ops-anatomy-kubernetes/","title":"쿠버네티스 내부구조 분석"}},{"node":{"path":"/operations/istio-traffic/","title":"[Service Mesh] Istio 를 통한 동적 트래픽 라우팅"}},{"node":{"path":"/operations/apply-security-to-12st-mall/","title":"12번가 Mall에 토큰인증 적용하기"}},{"node":{"path":"/operations/azure/","title":"Azure Cloud Setup (AKS, ACR 설정)"}},{"node":{"path":"/operations/end-to-end/","title":"12번가 전체 마이크로서비스의 배포"}},{"node":{"path":"/operations/istio-msa-telemetry/","title":"[Service Mesh] MSA 모니터링 w/ Istio addon Grafana"}},{"node":{"path":"/operations/istio-metric-based-hpa/","title":"[Service Mesh] Istio Metrics based HPA"}},{"node":{"path":"/operations/gitops-argo-cd/","title":"[GitOps] Argo CD 를 통한 카나리 배포"}},{"node":{"path":"/development/pubsub-idempotency/","title":"Pub/Sub 방식의 연동 - Choreography with Idempotency"}},{"node":{"path":"/development/understanding-jpa-based-single-microservice/","title":"마이크로서비스 구현 및 동작원리 이해"}},{"node":{"path":"/development/token-based-auth/","title":"JWT Token 기반 인증 인가"}},{"node":{"path":"/development/pubsub-deadline/","title":"Pub/Sub 방식의 연동 - Choreography with Deadline added"}},{"node":{"path":"/development/ops-docker/","title":"Application Packaging with Container (Docker)"}},{"node":{"path":"/development/oauth2with-keycloak/","title":"JWT Token 기반 인증 인가 - Advanced"}},{"node":{"path":"/development/pub-sub/","title":"Pub/Sub 방식의 연동 "}},{"node":{"path":"/development/dp-cqrs/","title":"Data Projection with CQRS"}},{"node":{"path":"/development/monolith-2-misvc/","title":"Req/Res 방식의 MSA 연동 "}},{"node":{"path":"/development/kafka-connect/","title":"CDC(Change Data Capture) with Kafka"}},{"node":{"path":"/development/kafka-scaling/","title":"Kafka Scaling "}},{"node":{"path":"/development/kafka-basic/","title":"Kafka 기본 명령어 "}},{"node":{"path":"/development/kafka-retry-dlq/","title":"Kafka Retry & Dead Letter Queue "}},{"node":{"path":"/development/dp-graphql/","title":"Data Projection with GraphQL"}},{"node":{"path":"/development/gateway/","title":"API Gateway"}},{"node":{"path":"/development/dp-frontend/","title":"Data Projection with Frontend and HATEOAS"}},{"node":{"path":"/development/contract-test/","title":"Contract Test (Consumer Driven Test)"}},{"node":{"path":"/development/cna-start/","title":"단위 마이크로 서비스의 실행 "}},{"node":{"path":"/development/compensation-correlation/","title":"Pub/Sub 방식의 연동 - Compensation 과 Correlation"}},{"node":{"path":"/business/ddd-google-drive/","title":"[이벤트스토밍] - 구글 드라이브 예제"}},{"node":{"path":"/business/","title":"[이벤트스토밍] - 12번가 쇼핑몰 예제"}},{"node":{"path":"/development/circuit-breaker/","title":"Req/Res 방식에서 장애전파 차단 - 서킷브레이커 "}},{"node":{"path":"/business/eventstorming-fooddelivery/","title":"[이벤트스토밍] - DDD Food Delivery 예제"}}]}},"context":{}}